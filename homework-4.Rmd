---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Jacob Dubbert"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(doMC) 
library(reshape2)
theme_set(theme_bw())
```

## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1
```{r}
set.seed(1)
df <- tbl_df(Boston)
inTraining <- createDataPartition(df$medv, p = .75, list = F)
training <- df[inTraining,]
testing <- df[-inTraining,]
```

```{r}
set.seed(1982)
trees <- seq(25,500, by=25)
mtry1 <- seq(3,9, by=1)
rf_boston_cv <- train(medv ~ .,
                      data = training,
                      method = "rf",
                      ntree = 500,
                      importance=T,
                      tuneGrid = data.frame(mtry = mtry1))
rf_boston_cv
```

```{r}
registerDoMC(cores = 4)

set.seed(1982)
ntrees <- seq(25,500, by=25)
mse_test <- tibble(ntree = ntrees,
                   mtry_3= 1:20,
                   mtry_4= 1:20,
                   mtry_5= 1:20,
                   mtry_6=1:20,
                   mtry_7=1:20,
                   mtry_8=1:20,
                   mtry_9=1:20)
mtry1 <- seq(3,9, by=1)


for (nti in 1:length(ntrees)) {
  rf_boston_cv2 <- train(medv ~ .,
                      data = training,
                      method = "rf",
                      ntree = ntrees[nti],
                      importance=T,
                      tuneGrid = data.frame(mtry = 3:9))
  mse_test[nti, 2:8] <- rf_boston_cv2$results$RMSE^2
}

mse_test <- melt(mse_test, id.vars="ntree")
ggplot(mse_test, aes(ntree, value, col=variable)) + geom_line() + geom_point()

```

## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 

a.
```{r}
carseats <- Carseats
set.seed(9823)
inTraining <- createDataPartition(carseats$Sales, p = .5, list = F)
training <- carseats[inTraining,]
testing <- carseats[-inTraining,]
```

b.
```{r}
r_tree <- rpart(Sales~., data = training)
r_tree
prp(r_tree)
plot(as.party(r_tree))
```

```{r}
pred_test <- predict(r_tree, newdata = testing)
mean((testing$Sales-pred_test)^2)
```

The test MSE is 4.48

c.
```{r}
set.seed(9823)
fit_control <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 10)

cv_boston_tree <- train(Sales ~.,
                        data = training,
                        method= "rpart2",
                        trControl=fit_control,
                        tuneGrid = data.frame(maxdepth= 1:10))
cv_boston_tree
plot(cv_boston_tree)
```
It looks like k=3 is the optimal level of tree complexity.

```{r}
test_preds_cv <- predict(cv_boston_tree, newdata = testing)
mean((testing$Sales-test_preds_cv)^2)
```
Pruning the tree increases the MSE to 4.99.

d.
```{r}
bag_carseats <- randomForest(Sales ~ ., data = training, mtry = 10, ntree=500,importance=TRUE)
bag_carseats

bag_predict <- predict(bag_carseats, newdata = testing)
mean((testing$Sales - bag_predict)^2)
```
Test MSE is 3.00
```{r}
imp <- data.frame(importance(bag_carseats)[,-2])
rn <- rownames(imp)
imp_carseats <- data.frame(variable=rn,
                           importance=imp$importance.bag_carseats.....2.) %>% arrange(desc(-importance)) %>% mutate(variable= factor(variable, variable))
p <- ggplot(imp_carseats, aes(variable, importance))
p+geom_col()+coord_flip()
```
Bagging improves the test MSE to 3 and ShelveLoc, Price and CompPrice are the most important predictors of Sale.

e.
```{r}
rf_carseats <- randomForest(Sales ~., data = training, mtry=3, ntree=500, importance=T)
rf_carseats
rf_predict <- predict(rf_carseats, newdata = testing)
mean((testing$Sales-rf_predict)^2)
```
The test MSE is 3.55

```{r}
rf_carseats_cv <- train(Sales ~ ., 
                      data = training,
                      method = "rf",
                      ntree = 500,
                      importance = T,
                      tuneGrid = data.frame(mtry = 1:10))
rf_carseats_cv
plot(rf_carseats_cv)
```
```{r}
rf_carseats7 <- randomForest(Sales ~., data = training, mtry=7, ntree=500, importance=T)
rf_carseats7
rf_predict7 <- predict(rf_carseats7, newdata = testing)
mean((testing$Sales-rf_predict7)^2)
```
The test MSE picking the best mtry through CV is 3.05.
```{r}
importance(rf_carseats7)
```
The test MSE is worse in this case for the Random Forest model barely from 3 in the Bagging model to 3.06 in the RF. Chaning m varies the test MSE between 3 and 3.61.Again, the most important predictors for Sales are ShelveLoc, Price and CompPrice. 

```{r}
imp <- varImp(rf_carseats_cv)$importance
rn <- rownames(imp)
imp_rf <- data.frame(variable=rn,
                     importance=imp$Overall) %>% 
  arrange(desc(-importance)) %>% 
  mutate(variable = factor(variable, variable))
p<- ggplot(imp_rf, aes(variable, importance))
p + geom_col()+coord_flip()
```

